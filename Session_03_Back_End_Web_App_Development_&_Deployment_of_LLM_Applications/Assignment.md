# ğŸ¤ Assignment: Build Your Full-Stack *Hot Mess Coach* (Backend + LLM + Optional Frontend)

## ğŸ“š Table of Contents

- [Overview](#overview)
- [Part 1 â€” Backend: Build FastAPI + LLM + Deploy to Vercel ğŸâ˜ï¸](#part-1--backend-build-fastapi--llm--deploy-to-vercel-ï¸)
  - [Step 1: Set Up Your Environment ğŸ—ï¸](#step-1-set-up-your-environment-ï¸)
  - [Step 2: Scaffold Your FastAPI Backend ğŸ§±](#step-2-scaffold-your-fastapi-backend-)
  - [Step 3: Add LLM Chat to FastAPI ğŸ¤–](#step-3-add-llm-chat-to-fastapi-)
  - [Step 4: Test Locally ğŸƒ](#step-4-test-locally-)
  - [Step 5: Deploy FastAPI to Vercel â˜ï¸](#step-5-deploy-fastapi-to-vercel-ï¸)
- [Part 2 â€” Advanced Build: Add Frontend to Your Backend (One-Shot Vibecoding) ğŸ¨âš¡](#part-2--advanced-build-add-frontend-to-your-backend-one-shot-vibecoding-)
  - [Step 6: Choose Your Frontend (Recommended: Hot Mess Frontend) ğŸ’…](#step-6-choose-your-frontend-recommended-hot-mess-frontend-)
  - [Step 7: Generate a New Frontend in v0 with Backend Awareness ğŸ§ â†’ğŸ¬](#step-7-generate-a-new-frontend-in-v0-with-backend-awareness-)
  - [Step 8: Connect Your Frontend â†’ Backend ğŸ”—](#step-8-connect-your-frontend--backend-)
  - [Step 9: Deploy the Frontend to Vercel ğŸŒ](#step-9-deploy-the-frontend-to-vercel-)
- [ğŸ—ï¸ Activity #3: Your Full-Stack Hot Mess Coach](#ï¸-activity-3-your-fullstack-hot-mess-coach)
- [ğŸ“ Tips for Success](#-tips-for-success)

---

# Overview

In this take-home assignment, you will:

1. **Build your own Python FastAPI backend**
2. **Add an LLM endpoint** so users can talk to â€œHot Mess Coachâ€
3. **Deploy the backend to Vercel**
4. (Advanced) **Create a frontend that calls your backend**
5. (Advanced) **Use one-shot vibecoding in v0 to generate a frontend that automatically integrates the backend**

---

# Part 1 â€” Backend: Build FastAPI + LLM + Deploy to Vercel ğŸâ˜ï¸

---

## Step 1: Set Up Your Environment ğŸ—ï¸

```bash
mkdir hot-mess-coach-backend
cd hot-mess-coach-backend
uv venv
source .venv/bin/activate
pip install fastapi uvicorn python-multipart openai
```

---

## Step 2: Scaffold Your FastAPI Backend ğŸ§±

Create `main.py`:

```python
from fastapi import FastAPI
app = FastAPI()

@app.get("/")
def home():
    return {"message": "Hot Mess Coach API is running ğŸš€"}

@app.get("/health")
def health():
    return {"status": "ok"}
```

Run:

```bash
uvicorn main:app --reload
```

---

## Step 3: Add LLM Chat to FastAPI ğŸ¤–

```python
from pydantic import BaseModel
from openai import OpenAI
client = OpenAI()

class ChatRequest(BaseModel):
    message: str

@app.post("/chat")
def chat(req: ChatRequest):
    completion = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "You are the Hot Mess Coach. A chaotic but helpful guide."},
            {"role": "user", "content": req.message}
        ]
    )
    return {"reply": completion.choices[0].message["content"]}
```

---

## Step 4: Test Locally ğŸƒ

```
uvicorn main:app --reload
```

Visit:

- http://localhost:8000  
- http://localhost:8000/docs  

---

## Step 5: Deploy FastAPI to Vercel â˜ï¸

Create `vercel.json`:

```json
{
  "builds": [{ "src": "main.py", "use": "@vercel/python" }],
  "routes": [{ "src": "/(.*)", "dest": "main.py" }]
}
```

Deploy:

```bash
vercel
```

---

# Part 2 â€” Advanced Build: Add Frontend to Your Backend (One-Shot Vibecoding) ğŸ¨âš¡

---

## Step 6: Choose Your Frontend (Recommended: Hot Mess Frontend) ğŸ’…

You may reuse the *Hot Mess Tracker* frontend from class or generate a new one.

---

## Step 7: Generate a New Frontend in v0 with Backend Awareness ğŸ§ â†’ğŸ¬

Example prompt:

```
Create me a frontend called â€œHot Mess Coach UIâ€.

This frontend must call my backend:
POST https://YOUR_BACKEND.vercel.app/chat

Body:
{ "message": "text" }

Use React + Tailwind + shadcn/ui.
Add playful chaotic UI elements.
Implement a chat interface that displays the LLM replies.
```

---

## Step 8: Connect Your Frontend â†’ Backend ğŸ”—

```ts
export async function sendMessage(message: string) {
  const res = await fetch("https://YOUR_BACKEND.vercel.app/chat", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({ message }),
  });
  return res.json();
}
```

Replace with your backend URL.

---

## Step 9: Deploy the Frontend to Vercel ğŸŒ

```bash
vercel
```

Your full-stack Hot Mess Coach is now live! ğŸ‰

---

# ğŸ—ï¸ Activity #3: Your Full-Stack Hot Mess Coach

### Required
- Build FastAPI backend  
- Add `/chat` endpoint  
- Deploy backend  

### Advanced
- Create frontend using v0 or reuse Hot Mess frontend  
- Connect to backend  
- Deploy frontend  

---

# ğŸ“ Tips for Success

- Use Cursor AI agents with your GitFlow rules  
- Test locally before deploying  
- Give v0 clear instructions (backend URL, endpoint, JSON format)  
- Keep your code clean and modular  

---
